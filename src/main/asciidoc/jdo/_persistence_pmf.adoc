[[pmf]]
= PersistenceManagerFactory
:_basedir: ../
:_imagesdir: images/

Any JDO-enabled application will require at least one _PersistenceManagerFactory_ (PMF)
http://www.datanucleus.org/javadocs/javax.jdo/3.2/javax/jdo/PersistenceManagerFactory.html[image:../images/javadoc.png[]]. 
Typically applications create one per datastore being utilised. 
A _PersistenceManagerFactory_ provides access to _PersistenceManager(s)_ which allow objects to be persisted, and retrieved.
The _PersistenceManagerFactory_ can be configured to provide particular behaviour.

CAUTION: A _PersistenceManagerFactory_ is designed to be thread-safe. A _PersistenceManager_ is not.

TIP: A _PersistenceyManagerFactory_ is expensive to create so you should create one per datastore for your application and retain it for as long as it is needed.
Always close your _PersistenceManagerFactory_ after you have finished with it.


There are many ways of creating a _PersistenceManagerFactory_, some of which are shown below

[source,java]
-----
Properties properties = new Properties();
properties.setProperty("javax.jdo.PersistenceManagerFactoryClass", "org.datanucleus.api.jdo.JDOPersistenceManagerFactory");
properties.setProperty("javax.jdo.option.ConnectionURL","jdbc:mysql://localhost/myDB");
properties.setProperty("javax.jdo.option.ConnectionDriverName","com.mysql.jdbc.Driver");
properties.setProperty("javax.jdo.option.ConnectionUserName","login");
properties.setProperty("javax.jdo.option.ConnectionPassword","password");
PersistenceManagerFactory pmf = JDOHelper.getPersistenceManagerFactory(properties);
-----

A slight variation on this, is to have a file to specify these properties in a file

-----
javax.jdo.PersistenceManagerFactoryClass=org.datanucleus.api.jdo.JDOPersistenceManagerFactory
javax.jdo.option.ConnectionURL=jdbc:mysql://localhost/myDB
javax.jdo.option.ConnectionDriverName=com.mysql.jdbc.Driver
javax.jdo.option.ConnectionUserName=login
javax.jdo.option.ConnectionPassword=password
-----

and then to create the _PersistenceManagerFactory_ using this file

[source,java]
-----
File propsFile = new File(filename);
PersistenceManagerFactory pmf = JDOHelper.getPersistenceManagerFactory(propsFile);
-----

or if the above file is in the CLASSPATH (at `datanucleus.properties` in the root of the CLASSPATH), then

[source,java]
-----
PersistenceManagerFactory pmf = JDOHelper.getPersistenceManagerFactory("datanucleus.properties");
-----

If using a _named PMF_ file, you can create the PMF by providing the link:#pmf_named[name of the PMF] like this

[source,java]
-----
PersistenceManagerFactory pmf = JDOHelper.getPersistenceManagerFactory("myNamedPMF");
-----

If using a `META-INF/persistence.xml` file, you can simply specify the link:#persistenceunit[persistence-unit] name as

[source,java]
-----
PersistenceManagerFactory pmf = JDOHelper.getPersistenceManagerFactory("myPersistenceUnit");
-----

Another alternative, when specifying your datastore via JNDI, would be to call _JDOHelper.getPersistenceManagerFactory(jndiLocation, context);_, 
and then set the other persistence properties on the received PMF.

Whichever way we wish to obtain the _PersistenceManagerFactory_ we have defined a series of properties to give the behaviour of the _PersistenceManagerFactory_. 
The first property specifies to use the DataNucleus implementation, and the following 4 properties define the datastore that it should connect to. 
There are many properties available. Some of these are standard JDO properties, and some are DataNucleus extensions.



[[persistenceunit]]
== PersistenceManagerFactory for Persistence-Unit

When designing an application you can usually nicely separate your persistable objects into independent groupings that can be treated separately, 
perhaps within a different DAO object, if using DAOs. JDO uses the (JPA) idea of a _persistence-unit_. 
A _persistence-unit_ provides a convenient way of specifying a set of metadata files, and classes, and jars that contain all classes to be persisted in a grouping. 
The persistence-unit is named, and the name is used for identifying it.
Consequently this name can then be used when defining what classes are to be enhanced, for example.

To define a _persistence-unit_ you first need to add a file `persistence.xml` to the `META-INF/` directory of the CLASSPATH (this may mean `WEB-INF/classes/META-INF` when using a 
web-application in such as Tomcat). This file will be used to define your _persistence-unit(s)_. Lets show an example

[source,xml]
-----
<?xml version="1.0" encoding="UTF-8" ?>
<persistence xmlns="http://xmlns.jcp.org/xml/ns/persistence"
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xsi:schemaLocation="http://xmlns.jcp.org/xml/ns/persistence
        http://xmlns.jcp.org/xml/ns/persistence/persistence_2_1.xsd" version="2.1">

    <!-- Online Store -->
    <persistence-unit name="OnlineStore">
        <class>mydomain.samples.metadata.store.Product</class>
        <class>mydomain.samples.metadata.store.Book</class>
        <class>mydomain.samples.metadata.store.CompactDisc</class>
        <class>mydomain.samples.metadata.store.Customer</class>
        <class>mydomain.samples.metadata.store.Supplier</class>
        <exclude-unlisted-classes/>
        <properties>
            <property name="datanucleus.ConnectionDriverName" value="org.h2.Driver"/>
            <property name="datanucleus.ConnectionURL" value="jdbc:h2:mem:datanucleus"/>
            <property name="datanucleus.ConnectionUserName" value="sa"/>
            <property name="datanucleus.ConnectionPassword" value=""/>
        </properties>
    </persistence-unit>

    <!-- Accounting -->
    <persistence-unit name="Accounting">
        <mapping-file>/mydomain/samples/metadata/accounts/package.jdo</mapping-file>
        <properties>
            <property name="datanucleus.ConnectionDriverName" value="org.h2.Driver"/>
            <property name="datanucleus.ConnectionURL" value="jdbc:h2:mem:datanucleus"/>
            <property name="datanucleus.ConnectionUserName" value="sa"/>
            <property name="datanucleus.ConnectionPassword" value=""/>
        </properties>
    </persistence-unit>

</persistence>
-----

In this example we have defined 2 _persistence-unit(s)_. 
The first has the name "OnlineStore" and contains 5 classes (annotated). 
The second has the name "Accounting" and contains a metadata file called `package.jdo` in a particular package (which will define the classes being part of that unit). 
This means that once we have defined this we can reference these _persistence-unit(s)_ in our persistence operations. 
You can find the XSD for `persistence.xml` http://xmlns.jcp.org/xml/ns/persistence/persistence_2_1.xsd[here].

There are several sub-elements of this `persistence.xml` file

* *provider* - Not used by JDO
* *jta-data-source* - JNDI name for JTA connections (make sure you set _transaction-type_ as *JTA* on the persistence-unit for this).
You can alternatively specify JDO standard *javax.jdo.option.ConnectionFactoryName* to the same end.
* *non-jta-data-source* - JNDI name for non-JTA connections.
You can alternatively specify JDO standard *javax.jdo.option.ConnectionFactory2Name* to the same end.
* *shared-cache-mode* - Defines the way the L2 cache will operate. ALL means all entities cached. NONE means no entities will be cached. ENABLE_SELECTIVE means only cache
the entities that are specified. DISABLE_SELECTIVE means cache all unless specified. UNSPECIFIED leaves it to DataNucleus.
* *validation-mode* - Defines the validation mode for Bean Validation. AUTO, CALLBACK or NONE.
* *jar-file* - name of a JAR file to scan for annotated classes to include in this persistence-unit.
* *mapping-file* - name of an XML "mapping" file containing persistence information to be included in this persistence-unit. 
This is the JDO XML Metadata file (_package.jdo_) (*not* the ORM XML Metadata file)
* *class* - name of an annotated class to include in this persistence-unit
* *properties* - properties defining the persistence factory to be used.
* *exclude-unlisted-classes* - when this is specified then it will only load metadata for the classes/mapping files listed.


=== Use with JDO

JDO accepts the "persistence-unit" name to be specified when creating the _PersistenceManagerFactory_, like this

[source,java]
-----
PersistenceManagerFactory pmf = JDOHelper.getPersistenceManagerFactory("MyPersistenceUnit");
-----

=== Metadata loading using persistence unit

When you specify a PMF using a `persistence.xml` it will load the metadata for all classes that are specified directly in the persistence unit,
as well as all classes defined in JDO XML metadata files that are specified directly in the persistence unit. 
If you don't have the _exclude-unlisted-classes_ set to true then it will also do a CLASSPATH scan to try to find any other *annotated* classes that are part of that persistence unit.
To set the CLASSPATH scanner to a custom version use the persistence property *datanucleus.metadata.scanner* and set it to the classname of the scanner class.


[[persistenceunit_dynamic]]
=== Dynamically generated Persistence-Unit

image:../images/nucleus_extension.png[]

DataNucleus allows an extension to JDO to dynamically create persistence-units at runtime.
Use the following code sample as a guide. Obviously any classes defined in the persistence-unit need to have been enhanced.

[source,java]
-----
import org.datanucleus.metadata.PersistenceUnitMetaData;
import org.datanucleus.api.jdo.JDOPersistenceManagerFactory;
 
PersistenceUnitMetaData pumd = new PersistenceUnitMetaData("dynamic-unit", "RESOURCE_LOCAL", null);
pumd.addClassName("mydomain.test.A");
pumd.setExcludeUnlistedClasses();
pumd.addProperty("javax.jdo.ConnectionURL", "jdbc:hsqldb:mem:nucleus");
pumd.addProperty("javax.jdo.ConnectionDriverName", "org.hsqldb.jdbcDriver");
pumd.addProperty("javax.jdo.ConnectionUserName", "sa");
pumd.addProperty("javax.jdo.ConnectionPassword", "");
pumd.addProperty("datanucleus.schema.autoCreateAll", "true");

PersistenceManagerFactory pmf = new JDOPersistenceManagerFactory(pumd, null);
-----

It should be noted that if you call _pumd.toString();_ then this returns the text that would have been found in a `persistence.xml` file.



[[pmf_named]]
== Named PersistenceManagerFactory

Typically applications create one PMF per datastore being utilised. An alternate to link:#persistenceunit[persistence-unit] is to use a *named PMF*, defined in a file
`META-INF/jdoconfig.xml` at the root of the CLASSPATH (this may mean `WEB-INF/classes/META-INF` when using a web-application).
Let's see an example of a `jdoconfig.xml`

[source,xml]
-----
<?xml version="1.0" encoding="utf-8"?>
<jdoconfig xmlns="http://xmlns.jcp.org/xml/ns/jdo/jdoconfig"
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xsi:schemaLocation="http://xmlns.jcp.org/xml/ns/jdo/jdoconfig
        http://xmlns.jcp.org/xml/ns/jdo/jdoconfig_3_2.xsd" version="3.2">

    <!-- Datastore Txn PMF -->
    <persistence-manager-factory name="Datastore">
        <property name="javax.jdo.PersistenceManagerFactoryClass" value="org.datanucleus.api.jdo.JDOPersistenceManagerFactory"/>
        <property name="javax.jdo.option.ConnectionURL" value="jdbc:mysql://localhost/datanucleus?useServerPrepStmts=false"/>
        <property name="javax.jdo.option.ConnectionDriverName" value="com.mysql.jdbc.Driver"/>
        <property name="javax.jdo.option.ConnectionUserName" value="datanucleus"/>
        <property name="javax.jdo.option.ConnectionPassword" value=""/>
        <property name="javax.jdo.option.Optimistic" value="false"/>
        <property name="datanucleus.schema.autoCreateAll" value="true"/>
    </persistence-manager-factory>

    <!-- Optimistic Txn PMF -->
    <persistence-manager-factory name="Optimistic">
        <property name="javax.jdo.PersistenceManagerFactoryClass" value="org.datanucleus.api.jdo.JDOPersistenceManagerFactory"/>
        <property name="javax.jdo.option.ConnectionURL" value="jdbc:mysql://localhost/datanucleus?useServerPrepStmts=false"/>
        <property name="javax.jdo.option.ConnectionDriverName" value="com.mysql.jdbc.Driver"/>
        <property name="javax.jdo.option.ConnectionUserName" value="datanucleus"/>
        <property name="javax.jdo.option.ConnectionPassword" value=""/>
        <property name="javax.jdo.option.Optimistic" value="true"/>
        <property name="datanucleus.schema.autoCreateAll" value="true"/>
    </persistence-manager-factory>

</jdoconfig>
-----

So in this example we have 2 named PMFs. The first is known by the name "Datastore" and utilises datastore transactions. 
The second is known by the name "Optimistic" and utilises optimistic transactions. 
You simply define all properties for the particular PMF within its specification block. And finally we instantiate our PMF like this

[source,java]
-----
PersistenceManagerFactory pmf = JDOHelper.getPersistenceManagerFactory("Optimistic");
-----

That's it. The PMF we are returned from JDOHelper will have all of the properties defined in `META-INF/jdoconfig.xml` under the name of "Optimistic".


[[pmf_properties]]
== PersistenceManagerFactory Properties

An PersistenceManagerFactory is very configurable, and DataNucleus provides many properties to tailor its behaviour to your persistence needs.


=== Specifying the datastore properties

With JDO you have 3 ways of specifying the datastore via persistence properties

* *Specify the connection URL/driverName/userName/password* and it will internally create a DataSource for this URL (with optional connection pooling). 
This is achieved by specifying *javax.jdo.option.ConnectionDriverName*, *javax.jdo.option.ConnectionURL*,
*javax.jdo.option.ConnectionUserName*, and *javax.jdo.option.ConnectionPassword*
* *Specify the JNDI name of the connectionFactory*. This is achieved by specifying *javax.jdo.option.ConnectionFactoryName*, and *javax.jdo.option.ConnectionFactory2Name* (for secondary operations)
* *Specify the DataSource of the connectionFactory*. This is achieved by specifying
*javax.jdo.option.ConnectionFactory*, and *javax.jdo.option.ConnectionFactory2* (for secondary operations)

The JNDI routes are typically only for use with RDBMS datastores.


[[pmf_props_jdo]]
=== Standard JDO Properties

[cols="2,6", options="header"]
|===
|Parameter
|Description + Values

|javax.jdo.PersistenceManagerFactoryClass
|The name of the PMF implementation. _org.datanucleus.api.jdo.JDOPersistenceManagerFactory_ *Only required if you have more than one JDO implementation in the CLASSPATH*

|javax.jdo.option.ConnectionFactory
|Instance of a connection factory for *transactional* connections. This is an alternative to specifying the ConnectionURL.
*Only for RDBMS*, and it must be an instance of javax.sql.DataSource. See link:persistence.html#datastore_connection[here]

|javax.jdo.option.ConnectionFactory2
|Instance of a connection factory for *nontransactional* connections. This is an alternative to specifying the ConnectionURL.
*Only for RDBMS*, and it must be an instance of javax.sql.DataSource. See link:persistence.html#datastore_connection[here]

|javax.jdo.option.ConnectionFactoryName
|The JNDI name for a connection factory for *transactional* connections. 
*Only for RDBMS*, and it must be a JNDI name that points to a javax.sql.DataSource object. See link:persistence.html#datastore_connection[here]

|javax.jdo.option.ConnectionFactory2Name
|The JNDI name for a connection factory for *nontransactional* connections. 
*Only for RDBMS*, and it must be a JNDI name that points to a javax.sql.DataSource object. See link:persistence.html#datastore_connection[here]

|javax.jdo.option.ConnectionDriverName
|The name of the driver to use for the DB (typically the JDBC driver class name for RDBMS datastores).

|javax.jdo.option.ConnectionURL
|URL specifying the datastore to use for persistence.
Note that this will define the *type of datastore* as well as the datastore itself. Please refer to link:../datastores/datastores.html[the datastores guides]
for the URL appropriate for the type of datastore you're using.

|javax.jdo.option.ConnectionUserName
|Username to use for connecting to the DB

|javax.jdo.option.ConnectionPassword
|Password to use for connecting to the DB. See *datanucleus.ConnectionPasswordDecrypter*for a way of providing an encrypted password here

|javax.jdo.option.IgnoreCache
|Whether to ignore the cache for queries. If the user sets this to _true_ then the query will evaluate in the datastore, but the instances returned will be formed
from the datastore; this means that if an instance has been modified and its datastore values match the query then the instance returned will *not* be the currently
cached (updated) instance, instead an instance formed using the datastore values. {true, *false*}

|javax.jdo.option.Multithreaded
|Whether to try to run the PM multithreaded.
*Note that this is only a hint to try to allow thread-safe operations on the PM*.
Users are always advised to run a PM as single threaded, since some operations are not currently locked and so could cause issues multi-threaded. 
{true, *false*}

|javax.jdo.option.Optimistic
|Whether to use link:persistence.html#locking_optimistic[optimistic transactions]. 
{true, *false*}

|javax.jdo.option.RetainValues
|Whether to suppress the clearing of values from persistent instances on transaction completion.
{true, *false*}

|javax.jdo.option.RestoreValues
|Whether persistent object have transactional field values restored when transaction rollback occurs.
{true, *false*}

|javax.jdo.option.DetachAllOnCommit
|Allows the user to select that when a transaction is committed all objects enlisted in that transaction will be automatically detached.
{true, *false*}

|javax.jdo.option.CopyOnAttach
|Whether, when attaching a detached object, we create an attached copy or simply migrate the detached object to attached state
{*true*, false}

|javax.jdo.option.PersistenceUnitName
|Name of a _persistence-unit_ to be found in a `persistence.xml` file (under META-INF) that defines the persistence properties to use
and the classes to use within the persistence process.

|javax.jdo.option.ServerTimeZoneID
|Id of the TimeZone under which the datastore server is running. If this is not specified or is set to null it is assumed that the 
datastore server is running in the same timezone as the JVM under which DataNucleus is running.

|javax.jdo.option.Name
|Name of the named PMF to use. Refers to a PMF defined in `META-INF/jdoconfig.xml`.

|javax.jdo.option.ReadOnly
|Whether the datastore is read-only or not (fixed in structure and contents).
{true, *false*}

|javax.jdo.option.TransactionType
|Type of transaction to use.
{RESOURCE_LOCAL, JTA}

|javax.jdo.option.TransactionIsolationLevel
|Select the default transaction isolation level for ALL PM factories. 
Some databases do not support all isolation levels, refer to your database documentation. Please refer to the link:persistence.html#transaction_isolation[transaction guide]
{read-uncommitted, *read-committed*, repeatable-read, serializable}

|javax.jdo.option.NontransactionalRead
|Whether to allow nontransactional reads
{false, *true*}

|javax.jdo.option.NontransactionalWrite
|Whether to allow nontransactional writes
{false, *true*}

|javax.jdo.option.DatastoreReadTimeoutMillis
|The timeout to apply to all reads (millisecs), e.g by query or by PM.getObjectById(). *Only applies if the underlying datastore supports it*
{*0*, A positive value (MILLISECONDS)}

|javax.jdo.option.DatastoreWriteTimeoutMillis
|The timeout to apply to all writes (millisecs). *Only applies if the underlying datastore supports it*
{*0*, A positive value (MILLISECONDS)}

|javax.jdo.option.Mapping
|Name for the ORM MetaData mapping files to use with this PMF. For example if this is set to "mysql" then the implementation looks for MetaData mapping files called 
`{classname}-mysql.orm` or `package-mysql.orm`. If this is not specified then the JDO implementation assumes that all is specified in the JDO MetaData file.

|javax.jdo.mapping.Catalog
|Name of the catalog to use by default for all classes persisted using this PMF.
This can be overridden in the MetaData where required, and is optional.
DataNucleus will prefix all table names with this catalog name if the RDBMS supports specification of catalog names in DDL.

|javax.jdo.mapping.Schema
|Name of the schema to use by default for all classes persisted using this PMF.
This can be overridden in the MetaData where required, and is optional.
DataNucleus will prefix all table names with this schema name if the RDBMS supports specification of schema names in DDL.
|===


[[pmf_props_dn_datastore]]
=== DataNucleus Datastore Properties

image:../images/nucleus_extension.png[]

DataNucleus provides the following properties for configuring the datastore used by the PersistenceManagerFactory.

[cols="2,6", options="header"]
|===
|Parameter
|Description + Values

|datanucleus.ConnectionURL
|URL specifying the datastore to use for persistence.
Note that this will define the *type of datastore* as well as the datastore itself. Please refer to link:../datastores/datastores.html[the datastores guide]
for the URL appropriate for the type of datastore you're using.

|datanucleus.ConnectionUserName
|Username to use for connecting to the DB

|datanucleus.ConnectionPassword
|Password to use for connecting to the DB.
See property *datanucleus.ConnectionPasswordDecrypter* for a way of providing an encrypted password here

|datanucleus.ConnectionDriverName
|The name of the (JDBC) driver to use for the DB (for RDBMS only).

|datanucleus.ConnectionFactory
|Instance of a connection factory for *transactional* connections. This is an alternative to *datanucleus.ConnectionURL*.
*Only for RDBMS*, and it must be an instance of javax.sql.DataSource. See link:persistence.html#datasource[Data Sources].

|datanucleus.ConnectionFactory2
|Instance of a connection factory for *nontransactional* connections. This is an alternative to *datanucleus.ConnectionURL*.
*Only for RDBMS*, and it must be an instance of javax.sql.DataSource. See link:persistence.html#datasource[Data Sources].

|datanucleus.ConnectionFactoryName
|The JNDI name for a connection factory for *transactional* connections. 
*Only for RDBMS*, and it must be a JNDI name that points to a javax.sql.DataSource object. See link:persistence.html#datasource[Data Sources].

|datanucleus.ConnectionFactory2Name
|The JNDI name for a connection factory for *nontransactional* connections. 
*Only for RDBMS*, and it must be a JNDI name that points to a javax.sql.DataSource object. See link:persistence.html#datasource[Data Sources].

|datanucleus.ConnectionPasswordDecrypter
|Name of a class that implements _org.datanucleus.store.ConnectionEncryptionProvider_
and should only be specified if the password is encrypted in the persistence properties
|===



[[pmf_props_dn_persistence]]
=== DataNucleus Persistence Properties

image:../images/nucleus_extension.png[]

DataNucleus provides the following properties for configuring general persistence handling used by the PersistenceManagerFactory.

[cols="2,6", options="header"]
|===
|Parameter
|Description + Values

|datanucleus.IgnoreCache
|Whether to ignore the cache for queries. If the user sets this to _true_ then the query will evaluate in the datastore, but the instances returned will be formed
from the datastore; this means that if an instance has been modified and its datastore values match the query then the instance returned will *not* be the currently
cached (updated) instance, instead an instance formed using the datastore values. {true, *false*}

|datanucleus.Multithreaded
|Whether to run the PM multithreaded.
*Note that this is only a hint to try to allow thread-safe operations on the PM*.
Users are always advised to run a PM as single threaded, since some operations are not currently locked and so could cause issues multi-threaded. 
{true, *false*}

|datanucleus.Optimistic
|Whether to use link:persistence.html#locking_optimistic[optimistic transactions]. 
{true, *false*}

|datanucleus.RetainValues
|Whether to suppress the clearing of values from persistent instances on transaction completion.
{true, *false*}

|datanucleus.RestoreValues
|Whether persistent object have transactional field values restored when transaction rollback occurs.
{true, *false*}

|datanucleus.Mapping
|Name for the ORM MetaData mapping files to use with this PMF. For example if this is set to "mysql" then the implementation looks for MetaData mapping files called 
`{classname}-mysql.orm` or `package-mysql.orm`. If this is not specified then the JDO implementation assumes that all is specified in the JDO MetaData file.

|datanucleus.mapping.Catalog
|Name of the catalog to use by default for all classes persisted using this PMF.
This can be overridden in the MetaData where required, and is optional.
DataNucleus will prefix all table names with this catalog name if the RDBMS supports specification of catalog names in DDL. _RDBMS datastores only_

|datanucleus.mapping.Schema
|Name of the schema to use by default for all classes persisted using this PMF.
This can be overridden in the MetaData where required, and is optional.
DataNucleus will prefix all table names with this schema name if the RDBMS supports specification of schema names in DDL. _RDBMS datastores only_

|datanucleus.TenantId
|String id to use as a discriminator on all persistable class tables to restrict data for the tenant using this application instance 
(aka link:persistence.html#multitenancy[multi-tenancy via discriminator]). _RDBMS, MongoDB, HBase, Neo4j, Cassandra datastores only_

|datanucleus.TenantProvider
|Instance of a class that implements _org.datanucleus.store.schema.MultiTenancyProvider_
which will return the tenant name to use for each call. _RDBMS, MongoDB, HBase, Neo4j, Cassandra datastores only_

|datanucleus.CurrentUser
|String defining the current user for the persistence process. Used by link:mapping.html#auditing[auditing]. _RDBMS datastores only_

|datanucleus.CurrentUserProvider
|Instance of a class that implements _org.datanucleus.store.schema.CurrentUserProvider_
which will return the current user to use for each call. Used by link:mapping.html#auditing[auditing]. _RDBMS datastores only_

|datanucleus.DetachAllOnCommit
|Allows the user to select that when a transaction is committed all objects enlisted in that transaction will be automatically detached.
{true, *false*}

|datanucleus.detachAllOnRollback
|Allows the user to select that when a transaction is rolled back all objects enlisted in that transaction will be automatically detached.
{true, *false*}

|datanucleus.CopyOnAttach
|Whether, when attaching a detached object, we create an attached copy or simply migrate the detached object to attached state
{*true*, false}

|datanucleus.allowAttachOfTransient
|When you call PM.makePersistent with a transient object (with PK fields set), if you enable this feature then it will first check for existence of an object in the datastore 
with the same identity and, if present, will merge into that object (rather than just trying to persist a new object).
{true, *false*}

|datanucleus.attachSameDatastore
|When attaching an object DataNucleus by default assumes that you're attaching to the same datastore as you detached from. 
DataNucleus does though allow you to attach to a different datastore (for things like replication). 
Set this to _false_ if you want to attach to a different datastore to what you detached from.
This property is also useful if you are attaching and want it to check for existence of the object in the datastore
before attaching, and create it if not present (_true_ assumes that the object exists).
{*true*, false}

|datanucleus.detachAsWrapped
|When detaching, any mutable second class objects (Collections, Maps, Dates etc) are typically detached as the basic form (so you can use them on client-side
of your application). This property allows you to select to detach as wrapped objects. It only works with "detachAllOnCommit" situations (not with detachCopy) currently
{true, *false*}

|datanucleus.DetachOnClose
|This allows the user to specify whether, when a PM is closed, that all objects in the L1 cache are automatically detached.
*Users are recommended to use the _datanucleus.DetachAllOnCommit_ wherever possible*. This will not work in JCA mode.
{*false*, true}

|datanucleus.detachmentFields
|When detaching you can control what happens to loaded/unloaded fields of the FetchPlan. 
The default for JDO is to load any unloaded fields of the current FetchPlan before detaching. 
You can also unload any loaded fields that are not in the current FetchPlan (so you only get the fields you require) as well as a combination of both options
{*load-fields*, unload-fields, load-unload-fields}

|datanucleus.maxFetchDepth
|Specifies the default maximum fetch depth to use for fetching operations.
The JDO spec defines a default of 1, meaning that only the first level of related objects will be fetched by default.
{-1, *1*, positive integer (non-zero)}

|datanucleus.detachedState
|Allows control over which mechanism to use to determine the fields to be detached.
By default DataNucleus uses the defined "fetch-groups". 
JPA doesn't have that (although it is an option with DataNucleus), so we also allow _loaded_ which will detach just the currently loaded fields, and _all_ which will
detach all fields of the object. Be careful with this option since it, when used with maxFetchDepth of -1 will detach a whole object graph!
{*fetch-groups*, all, loaded}

|datanucleus.ServerTimeZoneID
|Id of the TimeZone under which the datastore server is running. If this is not specified or is set to null it is assumed that the 
datastore server is running in the same timezone as the JVM under which DataNucleus is running.

|datanucleus.PersistenceUnitName
|Name of a _persistence-unit_ to be found in a `persistence.xml` file (under META-INF) that defines the persistence properties to use
and the classes to use within the persistence process.

|datanucleus.PersistenceUnitLoadClasses
|Used when we have specified the persistence-unit name for a PMF and where we want the datastore "tables" for all classes of that persistence-unit 
loading up into the StoreManager. Defaults to false since some databases are slow so such an operation would slow down the startup process.
{true, *false*}

|datanucleus.persistenceXmlFilename
|URL name of the `persistence.xml` file that should be used instead of using `META-INF/persistence.xml`.

|datanucleus.datastoreReadTimeout
|The timeout to apply to all reads (millisecs), e.g by query or by PM.getObjectById(). *Only applies if the underlying datastore supports it*
{*0*, A positive value (MILLISECONDS)}

|datanucleus.datastoreWriteTimeout
|The timeout to apply to all writes (millisecs), e.g by makePersistent, or by an update. *Only applies if the underlying datastore supports it*
{*0*, A positive value (MILLISECONDS)}

|datanucleus.singletonPMFForName
|Whether to only allow a singleton PMF for a particular name (the name can be either the name of the PMF in `jdoconfig.xml`, or the name of the persistence-unit).
If a subsequent request is made for a PMF with a name that already exists then a warning will be logged and the original PMF returned.
{true, *false*}

|datanucleus.allowListenerUpdateAfterInit
|Whether you want to be able to add/remove listeners on the JDO PMF after it is marked as not configurable (when the first PM is created). 
The default matches the JDO spec, not allowing changes to the listeners in use.
{true, *false*}

|datanucleus.jmxType
|Which JMX server to use when hooking into JMX. Please refer to the link:persistence.html#monitoring[Monitoring Guide]
{default, mx4j}

|datanucleus.deletionPolicy
|Allows the user to decide the policy when deleting objects. The default is "JDO2" which firstly checks if the field is dependent and if so deletes dependents, 
and then for others will null any foreign keys out. The problem with this option is that it takes no account of whether the user has also
defined <foreign-key> elements, so we provide a "DataNucleus" mode that does the dependent field part first and then if a FK element is defined 
will leave it to the FK in the datastore to perform any actions, and otherwise does the nulling.
{*JDO2*, DataNucleus}

|datanucleus.identityStringTranslatorType
|You can allow identities input to _pm.getObjectById(id)_ be translated into valid JDO ids if there is a suitable translator.
See link:../extensions/extensions.html#identitystringtranslator[Identity String Translator Plugin]

|datanucleus.identityKeyTranslatorType
|You can allow identities input to _pm.getObjectById(cls, key)_ be translated into valid JDO ids if there is a suitable key translator.
See link:../extensions/extensions.html#identitykeytranslator[Identity Key Translator Plugin]

|datanucleus.datastoreIdentityType
|Which "datastore-identity" class plugin to use to represent datastore identities. 
Refer to link:../extensions/extensions.html#store_datastoreidentity[Datastore Identity extensions] for details.
{*datanucleus*, kodo, xcalia, {user-supplied plugin}}

|datanucleus.executionContext.maxIdle
|Specifies the maximum number of ExecutionContext objects that are pooled ready for use
{*20*, integer value greater than 0}

|datanucleus.executionContext.reaperThread
|Whether to start a reaper thread that continually monitors the pool of ExecutionContext objects and frees them off after they have surpassed their expiration period
{*false*, true}

|datanucleus.executionContext.closeActiveTxAction
|Defines the action if a PM is closed and there is an active transaction present
{*rollback*, exception}

|datanucleus.objectProvider.className
|Class name for the ObjectProvider to use when managing object state. The default for RDBMS is ReferentialStateManagerImpl, and is StateManagerImpl for all other datastores.

|datanucleus.useImplementationCreator
|Whether to allow use of the implementation-creator (feature of JDO to dynamically create implementations of persistent interfaces). 
{*true*, false}

|datanucleus.manageRelationships
|This allows the user control over whether DataNucleus will try to manage bidirectional relations, correcting the input objects so that all relations are consistent.
This process runs when flush()/commit() is called. You can set it to _false_ if you always set both sides of a relation when persisting/updating.
{*true*, false}

|datanucleus.manageRelationshipsChecks
|This allows the user control over whether DataNucleus will make consistency checks on bidirectional relations. 
If "datanucleus.managedRelationships" is not selected then no checks are performed. 
If a consistency check fails at flush()/commit() then a JDOUserException is thrown.
You can set it to _false_ if you want to omit all consistency checks.
{*true*, false}

|datanucleus.persistenceByReachabilityAtCommit
|Whether to run the "persistence-by-reachability" algorithm at commit() time.
This means that objects that were reachable at a call to makePersistent() but that are no longer persistent will be removed from persistence.
For performance improvements, consider turning this off.
{*true*, false}

|datanucleus.classLoaderResolverName
|Name of a ClassLoaderResolver to use in class loading. DataNucleus provides a default that loosely follows the JDO specification for class loading. 
This property allows the user to override this with their own class better suited to their own loading requirements.
{*datanucleus*, {name of class-loader-resolver plugin}}

|datanucleus.primaryClassLoader
|Sets a primary classloader for situations where a primary classloader is not accessible. 
This ClassLoader is used when the class is not found in the default ClassLoader search path. 
As example, when the database driver is loaded by a different ClassLoader not in the ClassLoader search path for JDO specification.

|datanucleus.plugin.pluginRegistryClassName
|Name of a class that acts as registry for plug-ins.
This defaults to _org.datanucleus.plugin.NonManagedPluginRegistry_ (for when not using OSGi).
If you are within an OSGi environment you can set this to  _org.datanucleus.plugin.OSGiPluginRegistry_

|datanucleus.plugin.pluginRegistryBundleCheck
|Defines what happens when plugin bundles are found and are duplicated
{*EXCEPTION*, LOG, NONE}

|datanucleus.plugin.allowUserBundles
|Defines whether user-provided bundles providing DataNucleus extensions will be registered. This is only respected if used in a non-Eclipse OSGi environment.
{*true*, false}

|datanucleus.plugin.validatePlugins
|Defines whether a validation step should be performed checking for plugin dependencies etc. This is only respected if used in a non-Eclipse OSGi environment.
{*false*, true}

|datanucleus.findObject.validateWhenCached
|When a user calls getObjectById (JDO) and they request validation this allows the turning off of validation when an object is found in the (L2) cache.
Can be useful for performance reasons, but should be used with care.
{*true*, false}

|datanucleus.findObject.typeConversion
|When calling PM.getObjectById(Class, Object) the second argument really ought to be the exact type of the primary-key field. 
This property enables conversion of basic numeric types (Long, Integer, Short) to the appropriate numeric type (if the PK is a numeric type). 
{*true*, false}
|===



[[pmf_props_dn_schema]]
=== DataNucleus Schema Properties

image:../images/nucleus_extension.png[]

DataNucleus provides the following properties for configuring schema handling used by the PersistenceManagerFactory.

[cols="2,6", options="header"]
|===
|Parameter
|Description + Values

|datanucleus.schema.autoCreateAll
|Whether to automatically generate any schema, tables, columns, constraints that don't exist. 
Please refer to the link:persistence.html#schema[Schema Guide] for more details.
{true, *false*}

|datanucleus.schema.autoCreateDatabase
|Whether to automatically generate any database (catalog/schema) that doesn't exist. 
This depends very much on whether the datastore in question supports this operation. 
Please refer to the link:persistence.html#schema[Schema Guide] for more details.
{true, *false*}

|datanucleus.schema.autoCreateTables
|Whether to automatically generate any tables that don't exist. 
Please refer to the link:persistence.html#schema[Schema Guide] for more details.
{true, *false*}

|datanucleus.schema.autoCreateColumns
|Whether to automatically generate any columns that don't exist. Please refer to the link:persistence.html#schema[Schema Guide] for more details.
{true, *false*}

|datanucleus.schema.autoCreateConstraints
|Whether to automatically generate any constraints that don't exist. Please refer to the link:persistence.html#schema[Schema Guide] for more details.
{true, *false*}

|datanucleus.autoCreateWarnOnError
|Whether to only log a warning when errors occur during the auto-creation/validation process.
*Please use with care since if the schema is incorrect errors will likely come up later and this will postpone those error checks til later, when it may be too late!!*
{true, *false*}

|datanucleus.schema.validateAll
|Alias for defining *datanucleus.schema.validateTables*, *datanucleus.schema.validateColumns* and *datanucleus.schema.validateConstraints* as all true.
Please refer to the link:persistence.html#schema[Schema Guide] for more details.
{true, *false*}

|datanucleus.schema.validateTables
|Whether to validate tables against the persistence definition. Please refer to the link:persistence.html#schema[Schema Guide] for more details.
{true, *false*}

|datanucleus.schema.validateColumns
|Whether to validate columns against the persistence definition. This refers to the column detail structure and NOT to whether the column exists or not. 
Please refer to the link:persistence.html#schema[Schema Guide] for more details.
{true, *false*}

|datanucleus.schema.validateConstraints
|Whether to validate table constraints against the persistence definition. 
Please refer to the link:persistence.html#schema[Schema Guide] for more details.
{true, *false*}

|datanucleus.readOnlyDatastore
|Whether the datastore is read-only or not (fixed in structure and contents).
{true, *false*}

|datanucleus.readOnlyDatastoreAction
|What happens when a datastore is read-only and an object is attempted to be persisted.
{*EXCEPTION*, IGNORE}

|datanucleus.generateSchema.database.mode
|Whether to perform any schema generation to the database at startup.
Will process the schema for all classes that have metadata loaded at startup (i.e the classes specified in a persistence-unit).
{create, drop, drop-and-create, *none*}

|datanucleus.generateSchema.scripts.mode
|Whether to perform any schema generation into scripts at startup.
Will process the schema for all classes that have metadata loaded at startup (i.e the classes specified in a persistence-unit).
{create, drop, drop-and-create, *none*}

|datanucleus.generateSchema.scripts.create.target
|Name of the script file to write to if doing a "create" with the target as "scripts"
{*datanucleus-schema-create.ddl*, {filename}}

|datanucleus.generateSchema.scripts.drop.target
|Name of the script file to write to if doing a "drop" with the target as "scripts"
{*datanucleus-schema-drop.ddl*, {filename}}

|datanucleus.generateSchema.scripts.create.source
|Name of a script file to run to create tables. Can be absolute filename, or URL string

|datanucleus.generateSchema.scripts.drop.source
|Name of a script file to run to drop tables. Can be absolute filename, or URL string

|datanucleus.generateSchema.scripts.load
|Name of a script file to run to load data into the schema. Can be absolute filename, or URL string

|datanucleus.identifierFactory
|Name of the identifier factory to use when generating table/column names etc (RDBMS datastores only). 
See also the link:mapping.html#datastore_identifiers[Datastore Identifier Guide].
{datanucleus1, *datanucleus2*, jpox, jpa, {user-plugin-name}}

|datanucleus.identifier.namingFactory
|Name of the identifier NamingFactory to use when generating table/column names etc (non-RDBMS datastores).
{*datanucleus2*, jpa, {user-plugin-name}}

|datanucleus.identifier.case
|Which case to use in generated table/column identifier names. 
See also the link:mapping.html#datastore_identifiers[Datastore Identifier Guide].
RDBMS defaults to UPPERCASE. Cassandra defaults to lowercase
{UPPERCASE, LowerCase, MixedCase}

|datanucleus.identifier.wordSeparator
|Separator character(s) to use between words in generated identifiers. Defaults to "_" (underscore)

|datanucleus.identifier.tablePrefix
|Prefix to be prepended to all generated table names (if the identifier factory supports it)

|datanucleus.identifier.tableSuffix
|Suffix to be appended to all generated table names (if the identifier factory supports it)

|datanucleus.defaultInheritanceStrategy
|How to choose the inheritance strategy default for classes where no strategy has been specified. 
With _JDO2_ this will be "new-table" for base classes and "superclass-table" for subclasses.
With _TABLE_PER_CLASS_ this will be "new-table" for all classes.
{*JDO2*, TABLE_PER_CLASS}

|datanucleus.store.allowReferencesWithNoImplementations
|Whether we permit a reference field (1-1 relation) or collection of references where there are no defined implementations of the reference. 
False means that an exception will be thrown during schema generation for the field
{true, *false*}
|===


[[pmf_props_dn_transaction]]
=== DataNucleus Transaction Properties

image:../images/nucleus_extension.png[]

DataNucleus provides the following properties for configuring transaction handling used by the PersistenceManagerFactory.

[cols="2,6", options="header"]
|===
|Parameter
|Description + Values

|datanucleus.transaction.type
|Type of transaction to use. If running under JavaSE the default is RESOURCE_LOCAL, and if running under JavaEE the default is JTA.
{RESOURCE_LOCAL, JTA}

|datanucleus.transaction.isolation
|Select the default transaction isolation level for ALL PM factories. 
Some databases do not support all isolation levels, refer to your database documentation. Please refer to the link:persistence.html#transaction_isolation[transaction guide].
{read-uncommitted, *read-committed*, repeatable-read, serializable}

|datanucleus.transaction.jta.transactionManagerLocator
|Selects the locator to use when using JTA transactions so that DataNucleus can find the JTA TransactionManager.
If this isn't specified and using JTA transactions DataNucleus will search all available locators which could have a performance impact.
See link:../extensions/extensions.html#jta_locator[JTA Locator extension].
If specifying "custom_jndi" please also specify "datanucleus.transaction.jta.transactionManagerJNDI"
{*autodetect*, jboss, jonas, jotm, oc4j, orion, resin, sap, sun, weblogic, websphere, custom_jndi, alias of a JTA transaction locator}

|datanucleus.transaction.jta.transactionManagerJNDI
|Name of a JNDI location to find the JTA transaction manager from (when using JTA transactions). 
This is for the case where you know where it is located. If not used DataNucleus will try certain well-known locations

|datanucleus.transaction.nontx.read
|Whether to allow nontransactional reads
{false, *true*}

|datanucleus.transaction.nontx.write
|Whether to allow nontransactional writes
{false, *true*}

|datanucleus.transaction.nontx.atomic
|When a user invokes a nontransactional operation they can choose for these changes to go straight to the datastore (atomically) or to wait until either the next transaction commit, 
or close of the PM. Disable this if you want operations to be processed with the next real transaction.
{*true*, false}

|datanucleus.SerializeRead
|With datastore transactions you can apply locking to objects as they are read from the datastore. 
This setting applies as the default for all PMs obtained. You can also specify this on a per-transaction or per-query basis (which is often better to avoid deadlocks etc)
{true, *false*}

|datanucleus.flush.auto.objectLimit
|For use when using (DataNucleus) "AUTO" flush mode (see `datanucleus.flush.mode`) and is the limit on number of dirty objects before a flush to the datastore will be performed.
{*1*, positive integer}

|datanucleus.flush.mode
|Sets when persistence operations are flushed to the datastore.
_MANUAL_ means that operations will be sent only on flush()/commit(). 
_QUERY_ means that operations will be sent on flush()/commit() and just before query execution.
_AUTO_ means that operations will be sent immediately (auto-flush)
{MANUAL, QUERY, AUTO}

|datanucleus.flush.optimised
|Whether to use an "optimised" flush process, changing the order of persists for referential integrity (as used by RDBMS typically), 
or whether to just build a list of deletes, inserts and updates and do them in batches. RDBMS defaults to true, whereas other datastores default to false 
(due to not having referential integrity, so gaining from batching
{true, false}

|datanucleus.connectionPoolingType
|This property allows you to utilise a 3rd party software package for enabling connection pooling.
When using RDBMS you can select from DBCP2, C3P0, Proxool, BoneCP, etc. You must have the 3rd party jars in the CLASSPATH to use these options.
Please refer to the link:persistence.html#connection_pooling[Connection Pooling guide] for details.
{None, *dbcp2-builtin*, DBCP2, C3P0, Proxool, BoneCP, HikariCP, Tomcat, {others}}

|datanucleus.connectionPoolingType.nontx
|This property allows you to utilise a 3rd party software package for enabling connection pooling *for nontransactional connections* using a DataNucleus plugin.
If you don't specify this value but do define the above value then that is taken by default. Refer to the above property for more details.
{None, *dbcp2-builtin*, DBCP2, C3P0, Proxool, BoneCP, HikariCP, Tomcat, {others}}

|datanucleus.connection.nontx.releaseAfterUse
|Applies only to non-transactional connections and refers to whether to re-use (pool) the connection internally for later use. 
The default behaviour is to close any such non-transactional connection after use. If doing significant non-transactional processing
in your application then this may provide performance benefits, but be careful about the number of connections being held open (if one is held open per PM).
{*true*, false}

|datanucleus.connection.singleConnectionPerExecutionContext
|With a PM we normally allocate one connection for a transaction and close it after the transaction, then a different connection for nontransactional ops. 
This flag acts as a hint to the store plugin to obtain and retain a single connection throughout the lifetime of the PM.
{true, *false*}

|datanucleus.connection.resourceType
|Resource Type for primary connection
{JTA, RESOURCE_LOCAL}

|datanucleus.connection.resourceType2
|Resource Type for secondary connection
{JTA, RESOURCE_LOCAL}
|===


[[pmf_props_dn_cache]]
=== DataNucleus Cache Properties

image:../images/nucleus_extension.png[]

DataNucleus provides the following properties for configuring cache handling used by the PersistenceManagerFactory.

[cols="2,6", options="header"]
|===
|Parameter
|Description + Values

|datanucleus.cache.collections
|SCO collections can be used in 2 modes in DataNucleus. You can allow DataNucleus to cache the collections contents, 
or you can tell DataNucleus to access the datastore for every access of the SCO collection. The default is to use the cached collection.
{*true*, false}

|datanucleus.cache.collections.lazy
|When using cached collections/maps, the elements/keys/values can be loaded when the object is initialised, or can be loaded when accessed (lazy loading). 
The default is to use lazy loading when the field is not in the current fetch group, and to not use lazy loading when the field is in the current fetch group.
{true, false}

|datanucleus.cache.level1.type
|Name of the type of Level 1 cache to use. Defines the backing map.
See also the link:persistence.html#level1_cache[Level 1 Cache docs]
{*soft*, weak, strong, {your-plugin-name}}

|datanucleus.cache.level2.type
|Name of the type of Level 2 Cache to use. Can be used to interface with external caching products. Use "none" to turn off L2 caching.
See also the link:persistence.html#cache_level2[Level 2 Cache docs]
{none, *soft*, weak, javax.cache, coherence, ehcache, ehcacheclassbased, cacheonix, oscache, redis, spymemcached, xmemcached, {your-plugin-name}}

|datanucleus.cache.level2.mode
|The mode of operation of the L2 cache, deciding which entities are cached. The default (UNSPECIFIED) is the same as DISABLE_SELECTIVE.
See also the link:persistence.html#cache_level2[Level 2 Cache docs]
{NONE, ALL, ENABLE_SELECTIVE, DISABLE_SELECTIVE, *UNSPECIFIED*}

|datanucleus.cache.level2.storeMode
|Whether to use the L2 cache for storing values (set to "bypass" to not store within the context of the operation)
{*use*, bypass}

|datanucleus.cache.level2.retrieveMode
|Whether to use the L2 cache for retrieving values (set to "bypass" to not retrieve from L2 cache within the context of the operation, i.e go to the datastore)
{*use*, bypass}

|datanucleus.cache.level2.updateMode
|When the objects in the L2 cache should be updated. Defaults to updating at commit AND when fields are read from a datastore object
{*commit-and-datastore-read*, commit}

|datanucleus.cache.level2.cacheName
|Name of the cache. This is for use with plugins such as the Tangosol cache plugin for accessing the particular cache. 
Please refer to the link:persistence.html#cache_level2[Level 2 Cache docs]

|datanucleus.cache.level2.maxSize
|Max size for the L2 cache (supported by weak, soft, coherence, ehcache, ehcacheclassbased, javax.cache)
{*-1*, integer value}

|datanucleus.cache.level2.clearAtClose
|Whether the close of the L2 cache (when the PMF closes) should also clear out any objects from the underlying cache mechanism. 
By default it will clear objects out but if the user has configured an external cache product and wants to share objects across multiple PMFs then this can be set to false.
{*true*, false}

|datanucleus.cache.level2.batchSize
|When objects are added to the L2 cache at commit they are typically batched. This property sets the max size of the batch.
{*100*, integer value}

|datanucleus.cache.level2.expiryMillis
|Some caches (Cacheonix, Redis) allow specification of an expiration time for objects in the cache. 
This property is the expiry timeout in milliseconds (will be unset meaning use cache default).
{*-1*, integer value}

|datanucleus.cache.level2.readThrough
|With javax.cache L2 caches you can configure the cache to allow read-through
{*true*, false}

|datanucleus.cache.level2.writeThrough
|With javax.cache L2 caches you can configure the cache to allow write-through
{*true*, false}

|datanucleus.cache.level2.storeByValue
|With javax.cache L2 caches you can configure the cache to store by value (as opposed to by reference)
{*true*, false}

|datanucleus.cache.level2.statisticsEnabled
|With javax.cache L2 caches you can configure the cache to enable statistics gathering (accessible via JMX)
{*false*, true}

|datanucleus.cache.queryCompilation.type
|Type of cache to use for caching of generic query compilations
{none, *soft*, weak, strong, javax.cache, {your-plugin-name}}

|datanucleus.cache.queryCompilation.cacheName
|Name of cache for generic query compilation. Used by javax.cache variant.
{{your-cache-name}, *datanucleus-query-compilation*}

|datanucleus.cache.queryCompilationDatastore.type
|Type of cache to use for caching of datastore query compilations
{none, *soft*, weak, strong, javax.cache, {your-plugin-name}}

|datanucleus.cache.queryCompilationDatastore.cacheName
|Name of cache for datastore query compilation. Used by javax.cache variant.
{{your-cache-name}, *datanucleus-query-compilation-datastore*}

|datanucleus.cache.queryResults.type
|Type of cache to use for caching query results.
{none, *soft*, weak, strong, javax.cache, redis, spymemcached, xmemcached, cacheonix, {your-plugin-name}}

|datanucleus.cache.queryResults.cacheName
|Name of cache for caching the query results.
{*datanucleus-query*, {your-name}}

|datanucleus.cache.queryResults.clearAtClose
|Whether the close of the Query Results cache (when the PMF closes) should also clear out any objects from the underlying cache mechanism. 
By default it will clear query results out.
{*true*, false}

|datanucleus.cache.queryResults.maxSize
|Max size for the query results cache (supported by weak, soft, strong)
{*-1*, integer value}

|datanucleus.cache.queryResults.expiryMillis
|Expiry in milliseconds for objects in the query results cache (cacheonix, redis)
{*-1*, integer value}
|===



[[pmf_props_dn_validation]]
=== DataNucleus Bean Validation Properties

image:../images/nucleus_extension.png[]

DataNucleus provides the following properties for configuring bean validation handling used by the PersistenceManagerFactory.

[cols="2,6", options="header"]
|===
|Parameter
|Description + Values

|datanucleus.validation.mode
|Determines whether the automatic lifecycle event validation is in effect. {*auto*, callback, none}

|datanucleus.validation.group.pre-persist
|The classes to validation on pre-persist callback

|datanucleus.validation.group.pre-update
|The classes to validation on pre-update callback

|datanucleus.validation.group.pre-remove
|The classes to validation on pre-remove callback

|datanucleus.validation.factory
|The validation factory to use in validation
|===




[[pmf_props_dn_value_generation]]
=== DataNucleus Value Generation Properties

image:../images/nucleus_extension.png[]

DataNucleus provides the following properties for configuring value generation handling used by the PersistenceManagerFactory.

[cols="2,6", options="header"]
|===
|Parameter
|Description + Values

|datanucleus.valuegeneration.transactionAttribute*
|Whether to use the PM connection or open a new connection. Only used by value generators that require a connection to the datastore.
{*New*, UsePM}

|datanucleus.valuegeneration.transactionIsolation
|Select the default transaction isolation level for identity generation. Must have _datanucleus.valuegeneration.transactionAttribute_ set to _New_
Some databases do not support all isolation levels, refer to your database documentation and the link:persistence.html#transaction_isolation[transaction guide]
{read-uncommitted, *read-committed*, repeatable-read, serializable}

|datanucleus.valuegeneration.sequence.allocationSize
|If using JDO3.0 still and not specifying the size of your sequence, this acts as the default allocation size.
{10, (integer value)}

|datanucleus.valuegeneration.increment.allocationSize
|Sets the default allocation size for any "increment" value strategy. You can configure each member strategy individually but they fall back to this value if not set
{10, (integer value)}
|===


[[pmf_props_dn_metadata]]
=== DataNucleus Metadata Properties

image:../images/nucleus_extension.png[]

DataNucleus provides the following properties for configuring metadata handling used by the PersistenceManagerFactory.

[cols="2,6", options="header"]
|===
|Parameter
|Description + Values

|datanucleus.metadata.jdoFileExtension
|Suffix for JDO MetaData files. Provides the ability to override the default suffix and also to have one PMF with one suffix and another with a different suffix, 
hence allowing differing persistence of the same classes using different PMF's.
{*jdo*, {file suffix}}

|datanucleus.metadata.ormFileExtension
|Suffix for ORM MetaData files. Provides the ability to override the default suffix and also to have one PMF with one suffix and another with a different suffix, 
hence allowing differing persistence of the same classes using different PMF's.
{*orm*, {file suffix}}

|datanucleus.metadata.jdoqueryFileExtension
|Suffix for JDO Query MetaData files. Provides the ability to override the default suffix and also to have one PMF with one suffix and another with a different suffix, 
hence allowing differing persistence of the same classes using different PMF's.
{*jdoquery*, {file suffix}}

|datanucleus.metadata.alwaysDetachable
|Whether to treat all classes as detachable irrespective of input metadata. See also "alwaysDetachable" enhancer option.
{*false*, true}

|datanucleus.metadata.listener.object
|Property specifying a org.datanucleus.metadata.MetaDataListener object that will be registered at startup and will receive notification of all metadata load activity.
{*false*, true}

|datanucleus.metadata.ignoreMetaDataForMissingClasses
|Whether to ignore classes where metadata is specified. Default (false) is to throw an exception.
{*false*, true}

|datanucleus.metadata.xml.validate
|Whether to validate the MetaData file(s) for XML correctness (against the DTD) when parsing.
{true, *false*}

|datanucleus.metadata.xml.namespaceAware
|Whether to allow for XML namespaces in metadata files. The vast majority of sane people should not need this at all, but it's enabled by default to allow for those that do (since v3.2.3)
{*true*, false}

|datanucleus.metadata.allowXML
|Whether to allow XML metadata. Turn this off if not using any, for performance.
{*true*, false}

|datanucleus.metadata.allowAnnotations
|Whether to allow annotations metadata. Turn this off if not using any, for performance.
{*true*, false}

|datanucleus.metadata.allowLoadAtRuntime
|Whether to allow load of metadata at runtime. This is intended for the situation where you are handling persistence of a persistence-unit and only want the
classes explicitly specified in the persistence-unit.
{*true*, false}

|datanucleus.metadata.autoregistration
|Whether to use the JDO auto-registration of metadata. Turned on by default
{*true*, false}

|datanucleus.metadata.supportORM
|Whether to support "orm" mapping files. By default we use what the datastore plugin supports. 
This can be used to turn it off when the datastore supports it but we dont plan on using it (for performance)
{*true*, false}

|datanucleus.metadata.defaultNullable
|Whether the default nullability for the fields should be nullable or non-nullable when no metadata regarding field nullability is specified at field level. 
The default is nullable i.e. to allow null values.
{*true*, false}

|datanucleus.metadata.scanner
|Name of a class to use for scanning the classpath for persistent classes when using a `persistence.xml`.
The class must implement the interface _org.datanucleus.metadata.MetaDataScanner_

|datanucleus.metadata.useDiscriminatorForSingleTable
|With JPA the spec implies that all use of "single-table" inheritance will use a discriminator. 
DataNucleus up to and including 5.0.2 relied on the user defining the discriminator, whereas it now will add one if not supplied. Set this to _false_ to get behaviour as it was <= 5.0.2
{*true*, false}
|===


[[pmf_props_dn_autostart]]
=== DataNucleus Autostart Properties

image:../images/nucleus_extension.png[]

DataNucleus provides the following properties for configuring auto-start mechanism handling used by the PersistenceManagerFactory.

[cols="2,6", options="header"]
|===
|Parameter
|Description + Values

|datanucleus.autoStartMechanism
|How to initialise DataNucleus at startup. This allows DataNucleus to read in from some source the classes that it was persisting for this data store the previous time. 
_XML_ stores the information in an XML file for this purpose.
_SchemaTable_ (only for RDBMS) stores a table in the RDBMS for this purpose. 
_Classes_ looks at the property _datanucleus.autoStartClassNames_ for a list of classes.
_MetaData_ looks at the property _datanucleus.autoStartMetaDataFiles_ for a list of metadata files
The other option (default) is _None_ (start from scratch each time). 
Please refer to the link:persistence.html#autostart[Auto-Start Mechanism Guide] for more details.
*Alternatively just use `persistence.xml` to specify the classes and/or mapping files to load at startup.* Note also that "Auto-Start" is for RUNTIME use only (not during SchemaTool).
{*None*, XML, Classes, MetaData, SchemaTable}

|datanucleus.autoStartMechanismMode
|The mode of operation of the auto start mode. Currently there are 3 values. "Quiet" means that at startup if any errors are encountered, they are fixed quietly. 
"Ignored" means that at startup if any errors are encountered they are just ignored. 
"Checked" means that at startup if any errors are encountered they are thrown as exceptions.
{Checked, Ignored, *Quiet*}

|datanucleus.autoStartMechanismXmlFile
|Filename used for the XML file for AutoStart when using "XML" Auto-Start Mechanism

|datanucleus.autoStartClassNames
|This property specifies a list of classes (comma-separated) that are loaded at startup when using the "Classes" Auto-Start Mechanism.

|datanucleus.autoStartMetaDataFiles
|This property specifies a list of metadata files (comma-separated) that are loaded at startup when using the "MetaData" Auto-Start Mechanism.
|===




[[emf_props_dn_query]]
=== DataNucleus Query Properties

image:../images/nucleus_extension.png[]

DataNucleus provides the following properties for configuring query handling used by the PersistenceManagerFactory.

[cols="2,6", options="header"]
|===
|Parameter
|Description + Values

|datanucleus.query.flushBeforeExecution
|This property can enforce a flush to the datastore of any outstanding changes just before executing all queries. 
If using optimistic transactions any updates are typically held back until flush/commit and so the query would otherwise not take them into account.
{true, *false*}

|datanucleus.query.closeable
|When set to false (the default) will simply close all results when close() is called.
When set to true it will also close the query object making it unusable, releasing all resources as well. Also applies to a JDO Extent use of close().
{true, *false*}

|datanucleus.query.useFetchPlan
|Whether to use the FetchPlan when executing a JDOQL query.
The default is to use it which means that the relevant fields of the object will be retrieved. This allows the option of just retrieving the identity columns.
{*true*, false}

|datanucleus.query.compileOptimiseVarThis
|This optimisation will detect and try to fix a query clause like "var == this" (which is pointless). It is not very advanced but may help in some situations
{true, *false*}

|datanucleus.query.jdoql.allowAll
|javax.jdo.query.JDOQL queries are allowed by JDO only to run SELECT queries.
This extension permits to bypass this limitation so that DataNucleus extension bulk "update" and bulk "delete" can be run.
{*false*, true}

|datanucleus.query.sql.allowAll
|javax.jdo.query.SQL queries are allowed by JDO only to run SELECT queries. This extension permits to bypass this limitation (so for example can execute stored procedures).
{*false*, true}

|datanucleus.query.jpql.allowRange
|JPQL queries, by the JPA spec, do not allow specification of the range in the query string.
This extension to allow "RANGE x,y" after the ORDER BY clause of JPQL string queries.
{*false*, true}

|datanucleus.query.checkUnusedParameters
|Whether to check for unused input parameters and throw an exception if found.
The JDO spec requires this check and is a good guide to having misnamed a parameter name in the query for example.
{*true*, false}
|===



[[pmf_props_specific_query]]
=== DataNucleus Datastore-Specific Properties

image:../images/nucleus_extension.png[]

DataNucleus provides the following properties for configuring datastore-specific used by the PersistenceManagerFactory.

[cols="2,6", options="header"]
|===
|Parameter
|Description + Values

|datanucleus.rdbms.datastoreAdapterClassName
|This property allows you to supply the class name of the adapter to use for your datastore.
The default is not to specify this property and DataNucleus will autodetect the datastore type and use its own internal datastore adapter classes. 
This allows you to override the default behaviour where there maybe is some issue with the default adapter class.

|datanucleus.rdbms.useLegacyNativeValueStrategy
|This property changes the process for deciding the value strategy to use when the user has selected "native" to be like it was with DN version 3.0 and earlier, so using
"increment" and "uuid-hex".
{true, *false*}

|datanucleus.rdbms.statementBatchLimit
|Maximum number of statements that can be batched. The default is 50 and also applies to delete of objects.
Please refer to the link:../datastores/datastores.html#statement_batching[Statement Batching guide]
{integer value (0 = no batching)}

|datanucleus.rdbms.checkExistTablesOrViews
|Whether to check if the table/view exists. If false, it disables the automatic generation of tables that don't exist.
{*true*, false}

|datanucleus.rdbms.useDefaultSqlType
|This property applies for schema generation in terms of setting the default column "sql-type" (when you haven't defined it) and where
the JDBC driver has multiple possible "sql-type" for a "jdbc-type".
If the property is set to false, it will take the first provided "sql-type" from the JDBC driver.
If the property is set to true, it will take the "sql-type" that matches what the DataNucleus "plugin.xml" implies.
{*true*, false}

|datanucleus.rdbms.initializeColumnInfo
|Allows control over what column information is initialised when a table is loaded for the first time. 
By default info for all columns will be loaded. Unfortunately some RDBMS are particularly poor at returning this information so we allow reduced forms to just load the 
primary key column info, or not to load any.
{*ALL*, PK, NONE}

|datanucleus.rdbms.classAdditionMaxRetries
|The maximum number of retries when trying to find a class to persist or when validating a class.
{*3*, A positive integer}

|datanucleus.rdbms.constraintCreateMode
|How to determine the RDBMS constraints to be created. 
*DataNucleus* will automatically add foreign-keys/indices to handle all relationships, and will utilise the specified MetaData foreign-key information.
*JDO2* will only use the information in the MetaData file(s).
{*DataNucleus*, JDO2}

|datanucleus.rdbms.uniqueConstraints.mapInverse
|Whether to add unique constraints to the element table for a map inverse field.
{*true*, false}

|datanucleus.rdbms.discriminatorPerSubclassTable
|Property that controls if only the base class where the discriminator is defined will have a discriminator column
{*false*, true}

|datanucleus.rdbms.stringDefaultLength
|The default (max) length to use for all strings that don't have their column length defined in MetaData.
{*255*, A valid length}

|datanucleus.rdbms.stringLengthExceededAction
|Defines what happens when persisting a String field and its length exceeds the length of the underlying datastore column. The default is to throw an Exception. 
The other option is to truncate the String to the length of the datastore column.
{*EXCEPTION*, TRUNCATE}

|datanucleus.rdbms.useColumnDefaultWhenNull
|If an object is being persisted and a field (column) is null, the default behaviour is to look whether the column has a "default" value defined in the datastore and pass that in. 
You can turn this off and instead pass in NULL for the column by setting this property to _false_.
{*true*, false}

|datanucleus.rdbms.persistEmptyStringAsNull
|When persisting an empty string, should it be persisted as null in the datastore?
This is to allow for datastores such as Oracle that dont differentiate between null and empty string. 
If it is set to false and the datastore doesnt differentiate then a special character will be saved when storing an empty string (and interpreted when reading in).
{true, *false*}

|datanucleus.rdbms.query.fetchDirection
|The direction in which the query results will be navigated.
{*forward*, reverse, unknown}

|datanucleus.rdbms.query.resultSetType
|Type of ResultSet to create. Note 1) Not all JDBC drivers accept all options. The values correspond directly to the ResultSet options. 
Note 2) Not all java.util.List operations are available for scrolling result sets. An Exception is raised when unsupported operations are invoked.
{*forward-only*, scroll-sensitive, scroll-insensitive}

|datanucleus.rdbms.query.resultSetConcurrency
|Whether the ResultSet is readonly or can be updated. Not all JDBC drivers support all options.
The values correspond directly to the ResultSet options.
{*read-only*, updateable}

|datanucleus.rdbms.query.multivaluedFetch
|How any multi-valued field should be fetched in a query. 'exists' means use an EXISTS statement hence retrieving all elements for the queried objects in one SQL 
with EXISTS to select the affected owner objects. 'none' means don't fetch container elements.
{*exists*, none}

|datanucleus.rdbms.oracle.nlsSortOrder
|Sort order for Oracle String fields in queries (BINARY disables native language sorting). *Applicable for RDBMS only*
{*LATIN*, See Oracle documentation}

|datanucleus.rdbms.mysql.engineType
|Specify the default engine for any tables created in MySQL.
{*InnoDB*, valid engine for MySQL}

|datanucleus.rdbms.mysql.collation
|Specify the default collation for any tables created in MySQL.
{valid collation for MySQL}

|datanucleus.rdbms.mysql.characterSet
|Specify the default charset for any tables created in MySQL.
{valid charset for MySQL}

|datanucleus.rdbms.informix.useSerialForIdentity
|Whether we are using SERIAL for identity columns (instead of SERIAL8).
{true, *false*}

|datanucleus.rdbms.schemaTable.tableName
|Name of the table to use when using auto-start mechanism of "SchemaTable"
Please refer to the link:persistence.html#autostart[Auto-Start guide]
{NUCLEUS_TABLES, Valid table name}

|datanucleus.rdbms.dynamicSchemaUpdates
|Whether to allow dynamic updates to the schema. This means that upon each insert/update the types of objects will be tested and any previously unknown implementations of
interfaces will be added to the existing schema.
{true, *false*}

|datanucleus.rdbms.omitDatabaseMetaDataGetColumns
|Whether to bypass all calls to DatabaseMetaData.getColumns(). This JDBC method is called to get schema information, but on some JDBC drivers (e.g Derby) it can
take an inordinate amount of time. Setting this to true means that your datastore schema has to be correct and no checks will be performed.
{true, *false*}

|datanucleus.rdbms.sqlTableNamingStrategy
|Name of the plugin to use for defining the names of the aliases of tables in SQL statements.
{*alpha-scheme*, t-scheme}

|datanucleus.rdbms.tableColumnOrder
|How we should order the columns in a table. The default is to put the fields of the owning class first, followed by superclasses, then subclasses. 
An alternative is to start from the base superclass first, working down to the owner, then the subclasses
{*owner-first*, superclass-first}

|datanucleus.rdbms.allowColumnReuse
|This property allows you to reuse columns for more than 1 field of a class.
It is _false_ by default to protect the user from erroneously typing in a column name. Additionally, if a column is reused, the user ought to think about
how to determine which field is written to that column ... all reuse ought to imply the same value in those fields so it doesn't matter which field is written there, or
retrieved from there.
{true, *false*}

|datanucleus.rdbms.statementLogging
|How to log SQL statements. The default is to log the statement and replace any parameters with the value provided in angle brackets. 
Alternatively you can log the statement with any parameters replaced by just the values (no brackets). The final option is to log the raw JDBC statement (with ? for parameters).
{*values-in-brackets*, values, jdbc}

|datanucleus.rdbms.fetchUnloadedAutomatically
|If enabled will, upon a request to load a field, check for any unloaded fields that are non-relation fields or 1-1/N-1 fields and will load them in the same SQL call.
{true, *false*}

|datanucleus.cloud.storage.bucket
|This is a mandatory property that allows you to supply the bucket name to store your data. *Applicable for Google Storage, and AmazonS3 only.*

|datanucleus.hbase.relationUsesPersistableId
|This defines how relations will be persisted. The legacy method would be just to store the "id" of the object.
The default method is to use "persistableId" which is a form of the id but catering for datastore id and application id, and including the class of the target object to avoid subsequent lookups.
{*true*, false}

|datanucleus.hbase.enforceUniquenessInApplication
|Setting this property to true means that when a new object is persisted (and its identity is assigned), 
no check will be made as to whether it exists in the datastore and that the user takes responsibility for such checks.
{true, *false*}

|datanucleus.cassandra.enforceUniquenessInApplication
|Setting this property to true means that when a new object is persisted (and its identity is assigned), 
no check will be made as to whether it exists in the datastore (since Cassandra does an UPSERT) and that the user takes responsibility for such checks.
{true, *false*}

|datanucleus.cassandra.compression
|Type of compression to use for the Cassandra cluster.
{*none*, snappy}

|datanucleus.cassandra.metrics
|Whether metrics are enabled for the Cassandra cluster.
{*true*, false}

|datanucleus.cassandra.ssl
|Whether SSL is enabled for the Cassandra cluster.
{true, *false*}

|datanucleus.cassandra.socket.readTimeoutMillis
|Socket read timeout for the Cassandra cluster.

|datanucleus.cassandra.socket.connectTimeoutMillis
|Socket connect timeout for the Cassandra cluster.
|===


[[pmf_close]]
== Closing PersistenceManagerFactory

Since the PMF has significant resources associated with it, it should always be closed when you no longer need to perform any more persistence operations.
For most operations this will be when closing your application. Whenever it is you do it like this

[source,java]
-----
pmf.close();
-----



[[cache_level2]]
== Level 2 Cache

The _PersistenceManagerFactory_ has an optional cache of all objects across all _PersistenceManager_s.
This cache is called the Level 2 (L2) cache, and JDO doesn't define whether this should be enabled or not. With DataNucleus it defaults to enabled.
The user can configure the L2 cache if they so wish; by use of the persistence property *datanucleus.cache.level2.type*. You set this to "type" of cache required.
You currently have the following options.

* *soft* - use the internal (soft reference based) L2 cache. *This is the default L2 cache in DataNucleus.*
Provides support for the JDO interface of being able to put objects into the cache, and evict them when required.
This option does not support distributed caching, solely running within the JVM of the client application. 
Soft references are held to non pinned objects.
* *weak* - use the internal (weak reference based) L2 cache. 
Provides support for the JDO interface of being able to put objects into the cache, and evict them when required.
This option does not support distributed caching, solely running within the JVM of the client application. 
Weak references are held to non pinned objects.
* link:#cache_level2_javax_cache[javax.cache] - a simple wrapper to the Java standard "javax.cache" Temporary Caching API.
* link:#cache_level2_ehcache[EHCache] - a simple wrapper to EHCache's caching product.
* link:#cache_level2_ehcache[EHCacheClassBased] - similar to the EHCache option but class-based.
* link:#cache_level2_redis[Redis] - an L2 cache using Redis.
* link:#cache_level2_he[OSCache] - a simple wrapper to OSCache's caching product.
* link:#cache_level2_coherence[Oracle Coherence] - a simple wrapper to Oracle's Coherence caching product. 
Oracle's caches support distributed caching, so you could, in principle, use DataNucleus in a distributed environment with this option.
* link:#cache_level2_memcached[spymemcached] - a simple wrapper to the "spymemcached" client for memcached caching product.
* link:#cache_level2_memcached[xmemcached] - a simple wrapper to the "xmemcached" client for memcached caching product. 
* link:#cache_level2_cacheonix[cacheonix] - a simple wrapper to the Cacheonix distributed caching software.
* *none* - turn OFF Level 2 caching.

The weak, soft and javax.cache caches are available in the datanucleus-core plugin.
The EHCache, OSCache, Coherence, Cacheonix, and Memcache caches are available in the http://github.com/datanucleus/datanucleus-cache[datanucleus-cache] plugin.

In addition you can control the _mode_ of operation of the L2 cache. You do this using the persistence property *datanucleus.cache.level2.mode* (or *javax.persistence.sharedCache.mode*).
The default is _UNSPECIFIED_ which means that DataNucleus will cache all objects of entities unless the entity is explicitly marked as not cacheable. 
The other options are _NONE_ (don't cache ever), _ALL_ (cache all entities regardless of annotations),
_ENABLE_SELECTIVE_ (cache entities explicitly marked as cacheable), or _DISABLE_SELECTIVE_ (cache entities unless explicitly marked as not cacheable - i.e same as our default).

Objects are placed in the L2 cache when you commit() the transaction of a PersistenceManager. 
This means that you only have datastore-persisted objects in that cache. 
Also, if an object is deleted during a transaction then at commit it will be removed from the L2 cache if it is present.

link:../extensions/extensions.html#cache_level2[image:../images/nucleus_plugin.png[]]
The Level 2 cache is a DataNucleus plugin point allowing you to provide your own cache where you require it. Use the examples of the EHCache, Coherence caches etc as reference.

            
=== Controlling the Level 2 Cache

The majority of times when using a JDO-enabled system you will not have to take control over 
any aspect of the caching other than specification of whether to use a *Level 2* Cache or 
not. With JDO and DataNucleus you have the ability to control which objects remain in the cache. 
This is available via a method on the _PersistenceManagerFactory_.

[source,java]
-----
PersistenceManagerFactory pmf = JDOHelper.getPersistenceManagerFactory(props);
DataStoreCache cache = pmf.getDataStoreCache();
-----

The _DataStoreCache_ interface http://www.datanucleus.org/javadocs/javax.jdo/3.2/javax/jdo/datastore/DataStoreCache.html[image:../images/javadoc.png[Javadoc]]
provides methods to control the retention of objects in the cache. You have 3 groups of methods

* *evict* - used to remove objects from the Level 2 Cache
* *pin* - used to pin objects into the cache, meaning that they will not get removed by garbage collection, and will remain in the Level 2 cache until removed.
* *unpin* - used to reverse the effects of pinning an object in the Level 2 cache. This will mean that the object can thereafter be garbage collected if not being used.

These methods can be called to _pin_ objects into the cache that will be much used. Clearly 
this will be very much application dependent, but it provides a mechanism for users to exploit 
the caching features of JDO. If an object is not "pinned" into the L2 cache then it can typically 
be garbage collected at any time, so you should utilise the pinning capability for objects that 
you wish to retain access to during your application lifetime. For example, if you have an object 
that you want to be found from the cache you can do

[source,java]
-----
PersistenceManagerFactory pmf = JDOHelper.getPersistenceManagerFactory(props);
DataStoreCache cache = pmf.getDataStoreCache();
cache.pinAll(MyClass.class, false); // Pin all objects of type MyClass from now on
PersistenceManager pm = pmf.getPersistenceManager();
Transaction tx = pm.currentTransaction();
try
{
    tx.begin();

    pm.makePersistent(myObject);
    // "myObject" will now be pinned since we are pinning all objects of type MyClass.

    tx.commit();
}
finally
{
    if (tx.isActive())
    {
    	tx.close();
    }
}
-----

Thereafter, whenever something refers to _myObject_, it will find it in the Level 2 cache. 
To turn this behaviour off, the user can either *unpin* it or *evict* it.

JDO allows control over which classes are put into a Level 2 cache. You do this by specifying the *cacheable* attribute to _false_ (defaults to true).
So with the following specification, no objects of type _MyClass_ will be put in the L2 cache.

[source,java]
-----
@Cacheable("false")
public class MyClass
{
    ...
}
-----

or using XML metadata

[source,xml]
-----
<class name="MyClass" cacheable="false">
    ...
</class>
-----

JDO allows you control over which fields of an object are put in the Level 2 cache.
You do this by specifying the *cacheable* attribute to _false_ (defaults to true).
This setting is only required for fields that are relationships to other persistable objects. Like this

[source,java]
-----
public class MyClass
{
    ...

    Collection values;

    @Cacheable("false")
    Collection elements;
}
-----

or using XML metadata

[source,xml]
-----
<class name="MyClass">
    <field name="values"/>
    <field name="elements" cacheable="false"/>
    ...
</class>
-----

So in this example we will cache "values" but not "elements". If a field is _cacheable_ then

* If it is a persistable object, the "identity" of the related object will be stored in the Level 2 cache for this field of this object
* If it is a Collection of persistable elements, the "identity" of the elements will be stored in the Level 2 cache for this field of this object
* If it is a Map of persistable keys/values, the "identity" of the keys/values will be stored in the Level 2 cache for this field of this object

When pulling an object in from the Level 2 cache and it has a reference to another object DataNucleus uses the "identity" to find that object in the 
Level 1 or Level 2 caches to re-relate the objects.


[[cache_level2_javax_cache]]
=== L2 Cache using javax.cache

DataNucleus provides a simple wrapper to any compliant
http://jcp.org/en/jsr/detail?id=107[javax.cache implementation], for example
https://apacheignite.readme.io/[Apache Ignite] or https://hazelcast.org/[HazelCast].
To enable this you should put a "javax.cache" implementation in your CLASSPATH, and set the persistence properties

-----
datanucleus.cache.level2.type=javax.cache
datanucleus.cache.level2.cacheName={cache name}
-----

As an example, you could simply add the following to a Maven POM, together with those persistence properties above to use HazelCast "javax.cache" implementation

[source,xml]
-----
<dependency>
    <groupId>javax.cache</groupId>
    <artifactId>cache-api</artifactId>
    <version>1.0.0</version>
</dependency>
<dependency>
    <groupId>com.hazelcast</groupId>
    <artifactId>hazelcast</artifactId>
    <version>3.7.3</version>
</dependency>
-----


[[cache_level2_ehcache]]
=== L2 Cache using EHCache

DataNucleus provides a simple wrapper to http://www.sf.net/projects/ehcache[EHCache's own API caches] (not the javax.cache API variant). 
To enable this you should set the persistence properties

-----
datanucleus.cache.level2.type=ehcache
datanucleus.cache.level2.cacheName={cache name}
datanucleus.cache.level2.configurationFile={EHCache configuration file (in classpath)}
-----

The EHCache plugin also provides an alternative L2 Cache that is class-based. 
To use this you would need to replace "ehcache" above with "ehcacheclassbased".


[[cache_level2_memcached]]
=== L2 Cache using Spymemcached/Xmemcached

DataNucleus provides a simple wrapper to http://code.google.com/p/spymemcached/[Spymemcached caches] and http://code.google.com/p/xmemcached/[Xmemcached caches].
To enable this you should set the persistence properties

-----
datanucleus.cache.level2.type=spymemcached         [or "xmemcached"]
datanucleus.cache.level2.cacheName={prefix for keys, to avoid clashes with other memcached objects}
datanucleus.cache.level2.expireMillis=...
datanucleus.cache.level2.memcached.servers=...
-----

*datanucleus.cache.level2.memcached.servers* is a space separated list of memcached hosts/ports, e.g. host:port host2:port.
*datanucleus.cache.level2.expireMillis* if not set or set to 0 then no expire


[[cache_level2_cacheonix]]
=== L2 Cache using Cacheonix

DataNucleus provides a simple wrapper to http://www.cacheonix.com/[Cacheonix].
To enable this you should set the persistence properties

-----
datanucleus.cache.level2.type=cacheonix
datanucleus.cache.level2.cacheName={cache name}
-----

Note that you can optionally also specify

-----
datanucleus.cache.level2.expiryMillis={expiry-in-millis}
datanucleus.cache.level2.configurationFile={Cacheonix configuration file (in classpath)}
-----

and define a _cacheonix-config.xml_ like

[source,xml]
-----
<?xml version="1.0"?>
<cacheonix>
   <local>
      <!-- One cache per class being stored. -->
      <localCache name="mydomain.MyClass">
         <store>
            <lru maxElements="1000" maxBytes="1mb"/>
            <expiration timeToLive="60s"/>
         </store>
      </localCache>

      <!-- Fallback cache for classes indeterminable from their id. -->
      <localCache name="datanucleus">
         <store>
            <lru maxElements="1000" maxBytes="10mb"/>
            <expiration timeToLive="60s"/>
         </store>
      </localCache>

      <localCache name="default" template="true">
         <store>
            <lru maxElements="10" maxBytes="10mb"/>
            <overflowToDisk maxOverflowBytes="1mb"/>
            <expiration timeToLive="1s"/>
         </store>
      </localCache>
   </local>

</cacheonix>
-----


[[cache_level2_redis]]
=== L2 Cache using Redis

DataNucleus provides a simple L2 cache using Redis.
To enable this you should set the persistence properties

-----
datanucleus.cache.level2.type=redis
datanucleus.cache.level2.cacheName={cache name}
datanucleus.cache.level2.clearAtClose={true | false, whether to clear at close}
datanucleus.cache.level2.expiryMillis={expiry-in-millis}
datanucleus.cache.level2.redis.database={database, or use the default '1'}
datanucleus.cache.level2.redis.timeout={optional timeout, or use the default of 5000}
datanucleus.cache.level2.redis.sentinels={comma-separated list of sentinels, optional (use server/port instead)}
datanucleus.cache.level2.redis.server={server, or use the default of "localhost"}
datanucleus.cache.level2.redis.port={port, or use the default of 6379}
-----


[[cache_level2_oscache]]
=== L2 Cache using OSCache

DataNucleus provides a simple wrapper to http://www.opensymphony.com/oscache/[OSCache's caches]. 
To enable this you should set the persistence properties

-----
datanucleus.cache.level2.type=oscache
datanucleus.cache.level2.cacheName={cache name}
-----


[[cache_level2_coherence]]
=== L2 Cache using Oracle Coherence

DataNucleus provides a simple wrapper to http://www.oracle.com/technology/products/coherence/index.html[Oracle's Coherence caches].
This currently takes the _NamedCache_ interface in Coherence and instantiates a cache of a user provided name.
To enabled this you should set the following persistence properties

-----
datanucleus.cache.level2.type=coherence
datanucleus.cache.level2.cacheName={coherence cache name}
-----

The _Coherence cache name_ is the name that you would normally put into a call to CacheFactory.getCache(name). 
You have the benefits of Coherence's distributed/serialized caching. 
If you require more control over the Coherence cache whilst using it with DataNucleus, you can just access the cache directly via

[source,java]
-----
JDODataStoreCache cache = (JDODataStoreCache)pmf.getDataStoreCache();
NamedCache tangosolCache = ((TangosolLevel2Cache)cache.getLevel2Cache()).getTangosolCache();
-----
